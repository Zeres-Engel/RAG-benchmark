{
  "chunk-25bdedfba92a9a8adc3e77982f1ff724": {
    "tokens": 88,
    "content": "LightRAG is a simple and fast Retrieval-Augmented Generation system.\n            It uses a knowledge graph to efficiently store and retrieve information.\n            LightRAG supports multiple backends such as OpenAI, Ollama, and Hugging Face.\n            The system is particularly effective for natural language processing tasks.\n            In production, LightRAG can be bound to different storage backends such as PostgreSQL, Neo4j, or FAISS.",
    "chunk_order_index": 0,
    "full_doc_id": "doc-25bdedfba92a9a8adc3e77982f1ff724",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:8da9b1e1307cc9b9b79ad208cd1f521c",
      "default:extract:1a0367606e7b6877ae117f4504a50dcd"
    ],
    "create_time": 1758789051,
    "update_time": 1758789062,
    "_id": "chunk-25bdedfba92a9a8adc3e77982f1ff724"
  },
  "chunk-0588697e22866d45d52a5d33227b1e8c": {
    "tokens": 73,
    "content": "Knowledge graphs store entities and relationships. In LightRAG, entity extraction and relation merging\n            are performed asynchronously with configurable concurrency. A token budget divides the context window among\n            entities, relations, and chunks to produce balanced prompts for the LLM. Reranking can further refine retrieved\n            chunks, though lightweight environments may disable it for speed and simplicity.",
    "chunk_order_index": 0,
    "full_doc_id": "doc-0588697e22866d45d52a5d33227b1e8c",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:0e0dcd37fb8132534e289327be738ed4",
      "default:extract:86e6b0f7672c54fe4c265ead64c4f03b"
    ],
    "create_time": 1758789051,
    "update_time": 1758789062,
    "_id": "chunk-0588697e22866d45d52a5d33227b1e8c"
  },
  "chunk-b1a91ce3705c9370f88ef6024fdf1286": {
    "tokens": 64,
    "content": "Ollama serves local language and embedding models. For lightweight CPU or low-memory GPU environments,\n            gemma2:2b is a reasonable choice for generation and nomic-embed-text for embeddings. Adjusting the context size,\n            chunk size, and overlap reduces memory use while maintaining acceptable quality for small demos.",
    "chunk_order_index": 0,
    "full_doc_id": "doc-b1a91ce3705c9370f88ef6024fdf1286",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:4cb19463b8994dad2bedb0fcd795312d",
      "default:extract:81895045fc31b152bb1dc8eb4a2b5fcf"
    ],
    "create_time": 1758789062,
    "update_time": 1758789067,
    "_id": "chunk-b1a91ce3705c9370f88ef6024fdf1286"
  }
}